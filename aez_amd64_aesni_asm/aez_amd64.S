# Generated by PeachPy 0.2.0 from aez_amd64.py


#ifdef __APPLE__
.section __TEXT,__text,regular,pure_instructions
.globl _cpuidAMD64
.p2align 4, 0x90
_cpuidAMD64:
#else /* !__APPLE__ */
.text
.p2align 4,,15
.globl cpuidAMD64
.type cpuidAMD64, @function
cpuidAMD64:
#endif /* !__APPLE */
pushq %r15
pushq %rbx
	movq %rdi, %r15
	movl 0(%r15), %eax
	movl 8(%r15), %ecx
	cpuid
	movl %eax, 0(%r15)
	movl %ebx, 4(%r15)
	movl %ecx, 8(%r15)
	movl %edx, 12(%r15)
popq %rbx
popq %r15
	ret
#ifndef __APPLE__
.size cpuidAMD64, .-cpuidAMD64
#endif /* !__APPLE__ */

#ifdef __APPLE__
.section __TEXT,__text,regular,pure_instructions
.globl _resetAMD64SSE2
.p2align 4, 0x90
_resetAMD64SSE2:
#else /* !__APPLE__ */
.text
.p2align 4,,15
.globl resetAMD64SSE2
.type resetAMD64SSE2, @function
resetAMD64SSE2:
#endif /* !__APPLE */
	pxor %xmm0, %xmm0
	pxor %xmm1, %xmm1
	pxor %xmm2, %xmm2
	pxor %xmm3, %xmm3
	pxor %xmm4, %xmm4
	pxor %xmm5, %xmm5
	pxor %xmm6, %xmm6
	pxor %xmm7, %xmm7
	pxor %xmm8, %xmm8
	pxor %xmm9, %xmm9
	pxor %xmm10, %xmm10
	pxor %xmm10, %xmm11
	pxor %xmm12, %xmm12
	pxor %xmm13, %xmm13
	pxor %xmm14, %xmm14
	pxor %xmm15, %xmm15
	ret
#ifndef __APPLE__
.size resetAMD64SSE2, .-resetAMD64SSE2
#endif /* !__APPLE__ */

#ifdef __APPLE__
.section __TEXT,__text,regular,pure_instructions
.globl _xorBytes1x16AMD64SSE2
.p2align 4, 0x90
_xorBytes1x16AMD64SSE2:
#else /* !__APPLE__ */
.text
.p2align 4,,15
.globl xorBytes1x16AMD64SSE2
.type xorBytes1x16AMD64SSE2, @function
xorBytes1x16AMD64SSE2:
#endif /* !__APPLE */
	movdqu 0(%rdi), %xmm8
	movdqu 0(%rsi), %xmm9
	pxor %xmm9, %xmm8
	movdqu %xmm8, 0(%rdx)
	ret
#ifndef __APPLE__
.size xorBytes1x16AMD64SSE2, .-xorBytes1x16AMD64SSE2
#endif /* !__APPLE__ */

#ifdef __APPLE__
.section __TEXT,__text,regular,pure_instructions
.globl _xorBytes4x16AMD64SSE2
.p2align 4, 0x90
_xorBytes4x16AMD64SSE2:
#else /* !__APPLE__ */
.text
.p2align 4,,15
.globl xorBytes4x16AMD64SSE2
.type xorBytes4x16AMD64SSE2, @function
xorBytes4x16AMD64SSE2:
#endif /* !__APPLE */
	movdqu 0(%rdi), %xmm8
	movdqu 0(%rsi), %xmm9
	movdqu 0(%rdx), %xmm10
	movdqu 0(%rcx), %xmm11
	pxor %xmm9, %xmm8
	pxor %xmm11, %xmm10
	pxor %xmm10, %xmm8
	movdqu %xmm8, 0(%r8)
	ret
#ifndef __APPLE__
.size xorBytes4x16AMD64SSE2, .-xorBytes4x16AMD64SSE2
#endif /* !__APPLE__ */

#ifdef __APPLE__
.section __TEXT,__text,regular,pure_instructions
.globl _aezAES4AMD64AESNI
.p2align 4, 0x90
_aezAES4AMD64AESNI:
#else /* !__APPLE__ */
.text
.p2align 4,,15
.globl aezAES4AMD64AESNI
.type aezAES4AMD64AESNI, @function
aezAES4AMD64AESNI:
#endif /* !__APPLE */
	movdqu 0(%r8), %xmm8
	movdqa 0(%rdi), %xmm9
	movdqa 0(%rsi), %xmm10
	movdqa 0(%rdx), %xmm11
	pxor %xmm9, %xmm8
	pxor %xmm11, %xmm10
	pxor %xmm10, %xmm8
	pxor %xmm12, %xmm12
	movdqa 0(%rcx), %xmm10
	movdqa 16(%rcx), %xmm9
	movdqa 32(%rcx), %xmm11
	aesenc %xmm9, %xmm8
	aesenc %xmm10, %xmm8
	aesenc %xmm11, %xmm8
	aesenc %xmm12, %xmm8
	movdqu %xmm8, 0(%r9)
	ret
#ifndef __APPLE__
.size aezAES4AMD64AESNI, .-aezAES4AMD64AESNI
#endif /* !__APPLE__ */

#ifdef __APPLE__
.section __TEXT,__text,regular,pure_instructions
.globl _aezAES10AMD64AESNI
.p2align 4, 0x90
_aezAES10AMD64AESNI:
#else /* !__APPLE__ */
.text
.p2align 4,,15
.globl aezAES10AMD64AESNI
.type aezAES10AMD64AESNI, @function
aezAES10AMD64AESNI:
#endif /* !__APPLE */
	movdqu 0(%rdx), %xmm8
	movdqu 0(%rdi), %xmm11
	pxor %xmm11, %xmm8
	movdqa 0(%rsi), %xmm10
	movdqa 16(%rsi), %xmm9
	movdqa 32(%rsi), %xmm11
	aesenc %xmm10, %xmm8
	aesenc %xmm9, %xmm8
	aesenc %xmm11, %xmm8
	aesenc %xmm10, %xmm8
	aesenc %xmm9, %xmm8
	aesenc %xmm11, %xmm8
	aesenc %xmm10, %xmm8
	aesenc %xmm9, %xmm8
	aesenc %xmm11, %xmm8
	aesenc %xmm10, %xmm8
	movdqu %xmm8, 0(%rcx)
	ret
#ifndef __APPLE__
.size aezAES10AMD64AESNI, .-aezAES10AMD64AESNI
#endif /* !__APPLE__ */

#ifdef __APPLE__
.section __TEXT,__text,regular,pure_instructions
.globl _aezCorePass1AMD64AESNI
.p2align 4, 0x90
_aezCorePass1AMD64AESNI:
#else /* !__APPLE__ */
.text
.p2align 4,,15
.globl aezCorePass1AMD64AESNI
.type aezCorePass1AMD64AESNI, @function
aezCorePass1AMD64AESNI:
#endif /* !__APPLE */
	movq 16(%rsp), %rax
	movq $1, %r10
	movdqu 0(%rdx), %xmm11
	movdqu 0(%rcx), %xmm12
	movq %r9, %rcx
	movdqu 0(%rcx), %xmm9
	movdqu 16(%rcx), %xmm8
	movdqu 32(%rcx), %xmm10
	movq 8(%rsp), %rcx
	pxor %xmm15, %xmm15
	subq $256, %rax
	jb 371f # vector_loop256.end
203: # vector_loop256.begin:
		movdqu 16(%rdi), %xmm7
		movdqu 48(%rdi), %xmm6
		movdqu 80(%rdi), %xmm5
		movdqu 112(%rdi), %xmm4
		movdqu 144(%rdi), %xmm3
		movdqu 176(%rdi), %xmm2
		movdqu 208(%rdi), %xmm1
		movdqu 240(%rdi), %xmm0
		movdqa %xmm8, %xmm13
		pxor %xmm12, %xmm13
		pxor %xmm13, %xmm7
		pxor %xmm13, %xmm6
		pxor %xmm13, %xmm5
		pxor %xmm13, %xmm4
		pxor %xmm13, %xmm3
		pxor %xmm13, %xmm2
		pxor %xmm13, %xmm1
		pxor %xmm13, %xmm0
		pxor 16(%r8), %xmm7
		pxor 32(%r8), %xmm6
		pxor 48(%r8), %xmm5
		pxor 64(%r8), %xmm4
		pxor 80(%r8), %xmm3
		pxor 96(%r8), %xmm2
		pxor 112(%r8), %xmm1
		pxor 0(%r8), %xmm0
		aesenc %xmm8, %xmm7
		aesenc %xmm8, %xmm6
		aesenc %xmm8, %xmm5
		aesenc %xmm8, %xmm4
		aesenc %xmm8, %xmm3
		aesenc %xmm8, %xmm2
		aesenc %xmm8, %xmm1
		aesenc %xmm8, %xmm0
		aesenc %xmm9, %xmm7
		aesenc %xmm9, %xmm6
		aesenc %xmm9, %xmm5
		aesenc %xmm9, %xmm4
		aesenc %xmm9, %xmm3
		aesenc %xmm9, %xmm2
		aesenc %xmm9, %xmm1
		aesenc %xmm9, %xmm0
		aesenc %xmm10, %xmm7
		aesenc %xmm10, %xmm6
		aesenc %xmm10, %xmm5
		aesenc %xmm10, %xmm4
		aesenc %xmm10, %xmm3
		aesenc %xmm10, %xmm2
		aesenc %xmm10, %xmm1
		aesenc %xmm10, %xmm0
		aesenc %xmm15, %xmm7
		aesenc %xmm15, %xmm6
		aesenc %xmm15, %xmm5
		aesenc %xmm15, %xmm4
		aesenc %xmm15, %xmm3
		aesenc %xmm15, %xmm2
		aesenc %xmm15, %xmm1
		aesenc %xmm15, %xmm0
		movdqu 0(%rdi), %xmm13
		movdqu 32(%rdi), %xmm14
		pxor %xmm13, %xmm7
		pxor %xmm14, %xmm6
		movdqu 64(%rdi), %xmm13
		movdqu 96(%rdi), %xmm14
		pxor %xmm13, %xmm5
		pxor %xmm14, %xmm4
		movdqu 128(%rdi), %xmm13
		movdqu 160(%rdi), %xmm14
		pxor %xmm13, %xmm3
		pxor %xmm14, %xmm2
		movdqu 192(%rdi), %xmm13
		movdqu 224(%rdi), %xmm14
		pxor %xmm13, %xmm1
		pxor %xmm14, %xmm0
		movdqu %xmm7, 0(%rsi)
		movdqu %xmm6, 32(%rsi)
		movdqu %xmm5, 64(%rsi)
		movdqu %xmm4, 96(%rsi)
		movdqu %xmm3, 128(%rsi)
		movdqu %xmm2, 160(%rsi)
		movdqu %xmm1, 192(%rsi)
		movdqu %xmm0, 224(%rsi)
		pxor %xmm9, %xmm7
		pxor %xmm9, %xmm6
		pxor %xmm9, %xmm5
		pxor %xmm9, %xmm4
		pxor %xmm9, %xmm3
		pxor %xmm9, %xmm2
		pxor %xmm9, %xmm1
		pxor %xmm9, %xmm0
		aesenc %xmm8, %xmm7
		aesenc %xmm8, %xmm6
		aesenc %xmm8, %xmm5
		aesenc %xmm8, %xmm4
		aesenc %xmm8, %xmm3
		aesenc %xmm8, %xmm2
		aesenc %xmm8, %xmm1
		aesenc %xmm8, %xmm0
		aesenc %xmm9, %xmm7
		aesenc %xmm9, %xmm6
		aesenc %xmm9, %xmm5
		aesenc %xmm9, %xmm4
		aesenc %xmm9, %xmm3
		aesenc %xmm9, %xmm2
		aesenc %xmm9, %xmm1
		aesenc %xmm9, %xmm0
		aesenc %xmm10, %xmm7
		aesenc %xmm10, %xmm6
		aesenc %xmm10, %xmm5
		aesenc %xmm10, %xmm4
		aesenc %xmm10, %xmm3
		aesenc %xmm10, %xmm2
		aesenc %xmm10, %xmm1
		aesenc %xmm10, %xmm0
		aesenc %xmm15, %xmm7
		aesenc %xmm15, %xmm6
		aesenc %xmm15, %xmm5
		aesenc %xmm15, %xmm4
		aesenc %xmm15, %xmm3
		aesenc %xmm15, %xmm2
		aesenc %xmm15, %xmm1
		aesenc %xmm15, %xmm0
		movdqu 16(%rdi), %xmm13
		movdqu 48(%rdi), %xmm14
		pxor %xmm13, %xmm7
		pxor %xmm14, %xmm6
		movdqu 80(%rdi), %xmm13
		movdqu 112(%rdi), %xmm14
		pxor %xmm13, %xmm5
		pxor %xmm14, %xmm4
		movdqu 144(%rdi), %xmm13
		movdqu 176(%rdi), %xmm14
		pxor %xmm13, %xmm3
		pxor %xmm14, %xmm2
		movdqu 208(%rdi), %xmm13
		movdqu 240(%rdi), %xmm14
		pxor %xmm13, %xmm1
		pxor %xmm14, %xmm0
		movdqu %xmm7, 16(%rsi)
		movdqu %xmm6, 48(%rsi)
		movdqu %xmm5, 80(%rsi)
		movdqu %xmm4, 112(%rsi)
		movdqu %xmm3, 144(%rsi)
		movdqu %xmm2, 176(%rsi)
		movdqu %xmm1, 208(%rsi)
		movdqu %xmm0, 240(%rsi)
		pxor %xmm7, %xmm11
		pxor %xmm6, %xmm11
		pxor %xmm5, %xmm11
		pxor %xmm4, %xmm11
		pxor %xmm3, %xmm11
		pxor %xmm2, %xmm11
		pxor %xmm1, %xmm11
		pxor %xmm0, %xmm11
		movdqa 0(%rcx), %xmm13
		pshufb %xmm13, %xmm12
		movdqa %xmm12, %xmm14
		psrad $31, %xmm14
		pand 16(%rcx), %xmm14
		pshufd $147, %xmm14, %xmm14
		pslld $1, %xmm12
		pxor %xmm14, %xmm12
		pshufb %xmm13, %xmm12
		addq $256, %rdi
		addq $256, %rsi
		subq $256, %rax
		jae 203b # vector_loop256.begin
371: # vector_loop256.end:
	addq $256, %rax
	subq $128, %rax
	jb 457f # process_64bytes
	movdqu 16(%rdi), %xmm3
	movdqu 48(%rdi), %xmm2
	movdqu 80(%rdi), %xmm1
	movdqu 112(%rdi), %xmm0
	movdqa %xmm3, %xmm7
	movdqa %xmm2, %xmm6
	movdqu %xmm1, %xmm5
	movdqu %xmm0, %xmm4
	movdqa %xmm8, %xmm13
	pxor %xmm12, %xmm13
	pxor %xmm13, %xmm7
	pxor %xmm13, %xmm6
	pxor %xmm13, %xmm5
	pxor %xmm13, %xmm4
	pxor 16(%r8), %xmm7
	pxor 32(%r8), %xmm6
	pxor 48(%r8), %xmm5
	pxor 64(%r8), %xmm4
	aesenc %xmm8, %xmm7
	aesenc %xmm8, %xmm6
	aesenc %xmm8, %xmm5
	aesenc %xmm8, %xmm4
	aesenc %xmm9, %xmm7
	aesenc %xmm9, %xmm6
	aesenc %xmm9, %xmm5
	aesenc %xmm9, %xmm4
	aesenc %xmm10, %xmm7
	aesenc %xmm10, %xmm6
	aesenc %xmm10, %xmm5
	aesenc %xmm10, %xmm4
	aesenc %xmm15, %xmm7
	aesenc %xmm15, %xmm6
	aesenc %xmm15, %xmm5
	aesenc %xmm15, %xmm4
	movdqu 0(%rdi), %xmm13
	movdqu 32(%rdi), %xmm14
	pxor %xmm13, %xmm7
	pxor %xmm14, %xmm6
	movdqu 64(%rdi), %xmm13
	movdqu 96(%rdi), %xmm14
	pxor %xmm13, %xmm5
	pxor %xmm14, %xmm4
	movdqu %xmm7, 0(%rsi)
	movdqu %xmm6, 32(%rsi)
	movdqu %xmm5, 64(%rsi)
	movdqu %xmm4, 96(%rsi)
	pxor %xmm9, %xmm7
	pxor %xmm9, %xmm6
	pxor %xmm9, %xmm5
	pxor %xmm9, %xmm4
	aesenc %xmm8, %xmm7
	aesenc %xmm8, %xmm6
	aesenc %xmm8, %xmm5
	aesenc %xmm8, %xmm4
	aesenc %xmm9, %xmm7
	aesenc %xmm9, %xmm6
	aesenc %xmm9, %xmm5
	aesenc %xmm9, %xmm4
	aesenc %xmm10, %xmm7
	aesenc %xmm10, %xmm6
	aesenc %xmm10, %xmm5
	aesenc %xmm10, %xmm4
	aesenc %xmm15, %xmm7
	aesenc %xmm15, %xmm6
	aesenc %xmm15, %xmm5
	aesenc %xmm15, %xmm4
	pxor %xmm3, %xmm7
	pxor %xmm2, %xmm6
	pxor %xmm1, %xmm5
	pxor %xmm0, %xmm4
	movdqu %xmm7, 16(%rsi)
	movdqu %xmm6, 48(%rsi)
	movdqu %xmm5, 80(%rsi)
	movdqu %xmm4, 112(%rsi)
	pxor %xmm7, %xmm11
	pxor %xmm6, %xmm11
	pxor %xmm5, %xmm11
	pxor %xmm4, %xmm11
	addq $128, %rdi
	addq $128, %rsi
	addq $4, %r10
	subq $128, %rax
457: # process_64bytes:
	addq $128, %rax
	subq $64, %rax
	jb 508f # process_32bytes
	movq %r10, %rcx
	shlq $4, %rcx
	addq %r8, %rcx
	movdqu 16(%rdi), %xmm3
	movdqu 48(%rdi), %xmm2
	movdqa %xmm3, %xmm7
	movdqa %xmm2, %xmm6
	pxor %xmm8, %xmm7
	pxor %xmm8, %xmm6
	pxor %xmm12, %xmm7
	pxor %xmm12, %xmm6
	pxor 0(%rcx), %xmm7
	pxor 16(%rcx), %xmm6
	aesenc %xmm8, %xmm7
	aesenc %xmm8, %xmm6
	aesenc %xmm9, %xmm7
	aesenc %xmm9, %xmm6
	aesenc %xmm10, %xmm7
	aesenc %xmm10, %xmm6
	aesenc %xmm15, %xmm7
	aesenc %xmm15, %xmm6
	movdqu 0(%rdi), %xmm13
	movdqu 32(%rdi), %xmm14
	pxor %xmm13, %xmm7
	pxor %xmm14, %xmm6
	movdqu %xmm7, 0(%rsi)
	movdqu %xmm6, 32(%rsi)
	pxor %xmm9, %xmm7
	pxor %xmm9, %xmm6
	aesenc %xmm8, %xmm7
	aesenc %xmm8, %xmm6
	aesenc %xmm9, %xmm7
	aesenc %xmm9, %xmm6
	aesenc %xmm10, %xmm7
	aesenc %xmm10, %xmm6
	aesenc %xmm15, %xmm7
	aesenc %xmm15, %xmm6
	pxor %xmm3, %xmm7
	pxor %xmm2, %xmm6
	movdqu %xmm7, 16(%rsi)
	movdqu %xmm6, 48(%rsi)
	pxor %xmm7, %xmm11
	pxor %xmm6, %xmm11
	addq $64, %rdi
	addq $64, %rsi
	addq $2, %r10
	subq $64, %rax
508: # process_32bytes:
	addq $64, %rax
	subq $32, %rax
	jb 535f # out
	andq $7, %r10
	shlq $4, %r10
	addq %r10, %r8
	movdqu 16(%rdi), %xmm3
	movdqa %xmm3, %xmm7
	pxor %xmm8, %xmm7
	pxor %xmm12, %xmm7
	pxor 0(%r8), %xmm7
	aesenc %xmm8, %xmm7
	aesenc %xmm9, %xmm7
	aesenc %xmm10, %xmm7
	aesenc %xmm15, %xmm7
	movdqu 0(%rdi), %xmm13
	pxor %xmm13, %xmm7
	movdqu %xmm7, 0(%rsi)
	pxor %xmm9, %xmm7
	aesenc %xmm8, %xmm7
	aesenc %xmm9, %xmm7
	aesenc %xmm10, %xmm7
	aesenc %xmm15, %xmm7
	pxor %xmm3, %xmm7
	movdqu %xmm7, 16(%rsi)
	pxor %xmm7, %xmm11
535: # out:
	movdqu %xmm11, 0(%rdx)
	ret
#ifndef __APPLE__
.size aezCorePass1AMD64AESNI, .-aezCorePass1AMD64AESNI
#endif /* !__APPLE__ */

#ifdef __APPLE__
.section __TEXT,__text,regular,pure_instructions
.globl _aezCorePass2AMD64AESNI
.p2align 4, 0x90
_aezCorePass2AMD64AESNI:
#else /* !__APPLE__ */
.text
.p2align 4,,15
.globl aezCorePass2AMD64AESNI
.type aezCorePass2AMD64AESNI, @function
aezCorePass2AMD64AESNI:
#endif /* !__APPLE */
	movq 24(%rsp), %rax
	movq $1, %r10
	movq 8(%rsp), %r8
	movdqu 0(%r8), %xmm9
	movdqu 16(%r8), %xmm8
	movdqu 32(%r8), %xmm10
	movdqu 0(%rsi), %xmm12
	movdqu 0(%r8), %xmm13
	movq 16(%rsp), %r8
	pxor %xmm14, %xmm14
	movdqu 0(%rdx), %xmm11
	pxor 16(%rcx), %xmm11
	movq %rsp, %r11
	andq $18446744073709551584, %rsp
	subq $256, %rsp
	subq $256, %rax
	jb 825f # vector_loop256.end
571: # vector_loop256.begin:
		movdqa %xmm11, %xmm7
		pxor %xmm13, %xmm7
		movdqa %xmm7, %xmm6
		movdqa %xmm7, %xmm5
		movdqa %xmm7, %xmm4
		movdqa %xmm7, %xmm3
		movdqa %xmm7, %xmm2
		movdqa %xmm7, %xmm1
		movdqa %xmm7, %xmm0
		pxor 16(%r9), %xmm7
		pxor 32(%r9), %xmm6
		pxor 48(%r9), %xmm5
		pxor 64(%r9), %xmm4
		pxor 80(%r9), %xmm3
		pxor 96(%r9), %xmm2
		pxor 112(%r9), %xmm1
		pxor 0(%r9), %xmm0
		aesenc %xmm8, %xmm7
		aesenc %xmm8, %xmm6
		aesenc %xmm8, %xmm5
		aesenc %xmm8, %xmm4
		aesenc %xmm8, %xmm3
		aesenc %xmm8, %xmm2
		aesenc %xmm8, %xmm1
		aesenc %xmm8, %xmm0
		aesenc %xmm9, %xmm7
		aesenc %xmm9, %xmm6
		aesenc %xmm9, %xmm5
		aesenc %xmm9, %xmm4
		aesenc %xmm9, %xmm3
		aesenc %xmm9, %xmm2
		aesenc %xmm9, %xmm1
		aesenc %xmm9, %xmm0
		aesenc %xmm10, %xmm7
		aesenc %xmm10, %xmm6
		aesenc %xmm10, %xmm5
		aesenc %xmm10, %xmm4
		aesenc %xmm10, %xmm3
		aesenc %xmm10, %xmm2
		aesenc %xmm10, %xmm1
		aesenc %xmm10, %xmm0
		aesenc %xmm14, %xmm7
		aesenc %xmm14, %xmm6
		aesenc %xmm14, %xmm5
		aesenc %xmm14, %xmm4
		aesenc %xmm14, %xmm3
		aesenc %xmm14, %xmm2
		aesenc %xmm14, %xmm1
		aesenc %xmm14, %xmm0
		movdqu 0(%rdi), %xmm15
		movdqu 32(%rdi), %xmm11
		pxor %xmm7, %xmm15
		pxor %xmm6, %xmm11
		pxor %xmm15, %xmm12
		pxor %xmm11, %xmm12
		movdqa %xmm15, 0(%rsp)
		movdqa %xmm11, 32(%rsp)
		movdqu 64(%rdi), %xmm15
		movdqu 96(%rdi), %xmm11
		pxor %xmm5, %xmm15
		pxor %xmm4, %xmm11
		pxor %xmm15, %xmm12
		pxor %xmm11, %xmm12
		movdqa %xmm15, 64(%rsp)
		movdqa %xmm11, 96(%rsp)
		movdqu 128(%rdi), %xmm15
		movdqu 160(%rdi), %xmm11
		pxor %xmm3, %xmm15
		pxor %xmm2, %xmm11
		pxor %xmm15, %xmm12
		pxor %xmm11, %xmm12
		movdqa %xmm15, 128(%rsp)
		movdqa %xmm11, 160(%rsp)
		movdqu 192(%rdi), %xmm15
		movdqu 224(%rdi), %xmm11
		pxor %xmm1, %xmm15
		pxor %xmm0, %xmm11
		pxor %xmm15, %xmm12
		pxor %xmm11, %xmm12
		movdqa %xmm15, 192(%rsp)
		movdqa %xmm11, 224(%rsp)
		movdqu 16(%rdi), %xmm15
		movdqu 48(%rdi), %xmm11
		pxor %xmm15, %xmm7
		pxor %xmm11, %xmm6
		movdqa %xmm7, 16(%rsp)
		movdqa %xmm6, 48(%rsp)
		movdqu 80(%rdi), %xmm15
		movdqu 112(%rdi), %xmm11
		pxor %xmm15, %xmm5
		pxor %xmm11, %xmm4
		movdqa %xmm5, 80(%rsp)
		movdqa %xmm4, 112(%rsp)
		movdqu 144(%rdi), %xmm15
		movdqu 176(%rdi), %xmm11
		pxor %xmm15, %xmm3
		pxor %xmm11, %xmm2
		movdqa %xmm3, 144(%rsp)
		movdqa %xmm2, 176(%rsp)
		movdqu 208(%rdi), %xmm15
		movdqu 240(%rdi), %xmm11
		pxor %xmm15, %xmm1
		pxor %xmm11, %xmm0
		movdqa %xmm1, 208(%rsp)
		movdqa %xmm0, 240(%rsp)
		pxor %xmm9, %xmm7
		pxor %xmm9, %xmm6
		pxor %xmm9, %xmm5
		pxor %xmm9, %xmm4
		pxor %xmm9, %xmm3
		pxor %xmm9, %xmm2
		pxor %xmm9, %xmm1
		pxor %xmm9, %xmm0
		aesenc %xmm8, %xmm7
		aesenc %xmm8, %xmm6
		aesenc %xmm8, %xmm5
		aesenc %xmm8, %xmm4
		aesenc %xmm8, %xmm3
		aesenc %xmm8, %xmm2
		aesenc %xmm8, %xmm1
		aesenc %xmm8, %xmm0
		aesenc %xmm9, %xmm7
		aesenc %xmm9, %xmm6
		aesenc %xmm9, %xmm5
		aesenc %xmm9, %xmm4
		aesenc %xmm9, %xmm3
		aesenc %xmm9, %xmm2
		aesenc %xmm9, %xmm1
		aesenc %xmm9, %xmm0
		aesenc %xmm10, %xmm7
		aesenc %xmm10, %xmm6
		aesenc %xmm10, %xmm5
		aesenc %xmm10, %xmm4
		aesenc %xmm10, %xmm3
		aesenc %xmm10, %xmm2
		aesenc %xmm10, %xmm1
		aesenc %xmm10, %xmm0
		aesenc %xmm14, %xmm7
		aesenc %xmm14, %xmm6
		aesenc %xmm14, %xmm5
		aesenc %xmm14, %xmm4
		aesenc %xmm14, %xmm3
		aesenc %xmm14, %xmm2
		aesenc %xmm14, %xmm1
		aesenc %xmm14, %xmm0
		pxor 0(%rsp), %xmm7
		pxor 32(%rsp), %xmm6
		pxor 64(%rsp), %xmm5
		pxor 96(%rsp), %xmm4
		pxor 128(%rsp), %xmm3
		pxor 160(%rsp), %xmm2
		pxor 192(%rsp), %xmm1
		pxor 224(%rsp), %xmm0
		movdqu %xmm7, 16(%rdi)
		movdqu %xmm6, 48(%rdi)
		movdqu %xmm5, 80(%rdi)
		movdqu %xmm4, 112(%rdi)
		movdqu %xmm3, 144(%rdi)
		movdqu %xmm2, 176(%rdi)
		movdqu %xmm1, 208(%rdi)
		movdqu %xmm0, 240(%rdi)
		movdqa 0(%rcx), %xmm15
		pxor %xmm13, %xmm15
		pxor %xmm15, %xmm7
		pxor %xmm15, %xmm6
		pxor %xmm15, %xmm5
		pxor %xmm15, %xmm4
		pxor %xmm15, %xmm3
		pxor %xmm15, %xmm2
		pxor %xmm15, %xmm1
		pxor %xmm15, %xmm0
		pxor 16(%r9), %xmm7
		pxor 32(%r9), %xmm6
		pxor 48(%r9), %xmm5
		pxor 64(%r9), %xmm4
		pxor 80(%r9), %xmm3
		pxor 96(%r9), %xmm2
		pxor 112(%r9), %xmm1
		pxor 0(%r9), %xmm0
		aesenc %xmm8, %xmm7
		aesenc %xmm8, %xmm6
		aesenc %xmm8, %xmm5
		aesenc %xmm8, %xmm4
		aesenc %xmm8, %xmm3
		aesenc %xmm8, %xmm2
		aesenc %xmm8, %xmm1
		aesenc %xmm8, %xmm0
		aesenc %xmm9, %xmm7
		aesenc %xmm9, %xmm6
		aesenc %xmm9, %xmm5
		aesenc %xmm9, %xmm4
		aesenc %xmm9, %xmm3
		aesenc %xmm9, %xmm2
		aesenc %xmm9, %xmm1
		aesenc %xmm9, %xmm0
		aesenc %xmm10, %xmm7
		aesenc %xmm10, %xmm6
		aesenc %xmm10, %xmm5
		aesenc %xmm10, %xmm4
		aesenc %xmm10, %xmm3
		aesenc %xmm10, %xmm2
		aesenc %xmm10, %xmm1
		aesenc %xmm10, %xmm0
		aesenc %xmm14, %xmm7
		aesenc %xmm14, %xmm6
		aesenc %xmm14, %xmm5
		aesenc %xmm14, %xmm4
		aesenc %xmm14, %xmm3
		aesenc %xmm14, %xmm2
		aesenc %xmm14, %xmm1
		aesenc %xmm14, %xmm0
		pxor 16(%rsp), %xmm7
		pxor 48(%rsp), %xmm6
		pxor 80(%rsp), %xmm5
		pxor 112(%rsp), %xmm4
		pxor 144(%rsp), %xmm3
		pxor 176(%rsp), %xmm2
		pxor 208(%rsp), %xmm1
		pxor 240(%rsp), %xmm0
		movdqu %xmm7, 0(%rdi)
		movdqu %xmm6, 32(%rdi)
		movdqu %xmm5, 64(%rdi)
		movdqu %xmm4, 96(%rdi)
		movdqu %xmm3, 128(%rdi)
		movdqu %xmm2, 160(%rdi)
		movdqu %xmm1, 192(%rdi)
		movdqu %xmm0, 224(%rdi)
		movdqa 0(%r8), %xmm15
		pshufb %xmm15, %xmm13
		movdqa %xmm13, %xmm11
		psrad $31, %xmm11
		pand 16(%r8), %xmm11
		pshufd $147, %xmm11, %xmm11
		pslld $1, %xmm13
		pxor %xmm11, %xmm13
		pshufb %xmm15, %xmm13
		movdqu 0(%rdx), %xmm11
		pxor 16(%rcx), %xmm11
		addq $256, %rdi
		subq $256, %rax
		jae 571b # vector_loop256.begin
		movdqa %xmm14, 16(%rsp)
		movdqa %xmm14, 48(%rsp)
		movdqa %xmm14, 80(%rsp)
		movdqa %xmm14, 112(%rsp)
		movdqa %xmm14, 128(%rsp)
		movdqa %xmm14, 144(%rsp)
		movdqa %xmm14, 160(%rsp)
		movdqa %xmm14, 176(%rsp)
		movdqa %xmm14, 192(%rsp)
		movdqa %xmm14, 208(%rsp)
		movdqa %xmm14, 224(%rsp)
		movdqa %xmm14, 240(%rsp)
825: # vector_loop256.end:
	addq $256, %rax
	subq $128, %rax
	jb 953f # process_64bytes
	movdqa %xmm11, %xmm7
	pxor %xmm13, %xmm7
	movdqa %xmm7, %xmm6
	movdqa %xmm7, %xmm5
	movdqa %xmm7, %xmm4
	pxor 16(%r9), %xmm7
	pxor 32(%r9), %xmm6
	pxor 48(%r9), %xmm5
	pxor 64(%r9), %xmm4
	aesenc %xmm8, %xmm7
	aesenc %xmm8, %xmm6
	aesenc %xmm8, %xmm5
	aesenc %xmm8, %xmm4
	aesenc %xmm9, %xmm7
	aesenc %xmm9, %xmm6
	aesenc %xmm9, %xmm5
	aesenc %xmm9, %xmm4
	aesenc %xmm10, %xmm7
	aesenc %xmm10, %xmm6
	aesenc %xmm10, %xmm5
	aesenc %xmm10, %xmm4
	aesenc %xmm14, %xmm7
	aesenc %xmm14, %xmm6
	aesenc %xmm14, %xmm5
	aesenc %xmm14, %xmm4
	movdqu 0(%rdi), %xmm3
	movdqu 32(%rdi), %xmm1
	movdqu 64(%rdi), %xmm2
	movdqu 96(%rdi), %xmm0
	pxor %xmm7, %xmm3
	pxor %xmm6, %xmm1
	pxor %xmm5, %xmm2
	pxor %xmm4, %xmm0
	pxor %xmm3, %xmm12
	pxor %xmm1, %xmm12
	pxor %xmm2, %xmm12
	pxor %xmm0, %xmm12
	movdqa %xmm3, 0(%rsp)
	movdqa %xmm1, 32(%rsp)
	movdqa %xmm2, 64(%rsp)
	movdqa %xmm0, 96(%rsp)
	movdqu 16(%rdi), %xmm2
	movdqu 48(%rdi), %xmm0
	movdqu 80(%rdi), %xmm3
	movdqu 112(%rdi), %xmm1
	pxor %xmm2, %xmm7
	pxor %xmm0, %xmm6
	pxor %xmm3, %xmm5
	pxor %xmm1, %xmm4
	movdqu %xmm7, 16(%rdi)
	movdqu %xmm6, 48(%rdi)
	movdqu %xmm5, 80(%rdi)
	movdqu %xmm4, 112(%rdi)
	movdqa %xmm7, %xmm2
	movdqa %xmm6, %xmm0
	movdqa %xmm5, %xmm3
	movdqa %xmm4, %xmm1
	pxor %xmm9, %xmm7
	pxor %xmm9, %xmm6
	pxor %xmm9, %xmm5
	pxor %xmm9, %xmm4
	aesenc %xmm8, %xmm7
	aesenc %xmm8, %xmm6
	aesenc %xmm8, %xmm5
	aesenc %xmm8, %xmm4
	aesenc %xmm9, %xmm7
	aesenc %xmm9, %xmm6
	aesenc %xmm9, %xmm5
	aesenc %xmm9, %xmm4
	aesenc %xmm10, %xmm7
	aesenc %xmm10, %xmm6
	aesenc %xmm10, %xmm5
	aesenc %xmm10, %xmm4
	aesenc %xmm14, %xmm7
	aesenc %xmm14, %xmm6
	aesenc %xmm14, %xmm5
	aesenc %xmm14, %xmm4
	pxor 0(%rsp), %xmm7
	pxor 32(%rsp), %xmm6
	pxor 64(%rsp), %xmm5
	pxor 96(%rsp), %xmm4
	movdqu %xmm7, 16(%rdi)
	movdqu %xmm6, 48(%rdi)
	movdqu %xmm5, 80(%rdi)
	movdqu %xmm4, 112(%rdi)
	pxor 0(%rcx), %xmm7
	pxor 0(%rcx), %xmm6
	pxor 0(%rcx), %xmm5
	pxor 0(%rcx), %xmm4
	pxor %xmm13, %xmm7
	pxor %xmm13, %xmm6
	pxor %xmm13, %xmm5
	pxor %xmm13, %xmm4
	pxor 16(%r9), %xmm7
	pxor 32(%r9), %xmm6
	pxor 48(%r9), %xmm5
	pxor 64(%r9), %xmm4
	aesenc %xmm8, %xmm7
	aesenc %xmm8, %xmm6
	aesenc %xmm8, %xmm5
	aesenc %xmm8, %xmm4
	aesenc %xmm9, %xmm7
	aesenc %xmm9, %xmm6
	aesenc %xmm9, %xmm5
	aesenc %xmm9, %xmm4
	aesenc %xmm10, %xmm7
	aesenc %xmm10, %xmm6
	aesenc %xmm10, %xmm5
	aesenc %xmm10, %xmm4
	aesenc %xmm14, %xmm7
	aesenc %xmm14, %xmm6
	aesenc %xmm14, %xmm5
	aesenc %xmm14, %xmm4
	pxor %xmm2, %xmm7
	pxor %xmm0, %xmm6
	pxor %xmm3, %xmm5
	pxor %xmm1, %xmm4
	movdqu %xmm7, 0(%rdi)
	movdqu %xmm6, 32(%rdi)
	movdqu %xmm5, 64(%rdi)
	movdqu %xmm4, 96(%rdi)
	addq $128, %rdi
	addq $4, %r10
	subq $128, %rax
953: # process_64bytes:
	addq $128, %rax
	subq $64, %rax
	jb 1022f # process_32bytes
	movq %r10, %r8
	shlq $4, %r8
	addq %r9, %r8
	movdqa %xmm11, %xmm7
	pxor %xmm13, %xmm7
	movdqa %xmm7, %xmm6
	pxor 0(%r8), %xmm7
	pxor 16(%r8), %xmm6
	aesenc %xmm8, %xmm7
	aesenc %xmm8, %xmm6
	aesenc %xmm9, %xmm7
	aesenc %xmm9, %xmm6
	aesenc %xmm10, %xmm7
	aesenc %xmm10, %xmm6
	aesenc %xmm14, %xmm7
	aesenc %xmm14, %xmm6
	movdqu 0(%rdi), %xmm3
	movdqu 16(%rdi), %xmm2
	movdqu 32(%rdi), %xmm1
	movdqu 48(%rdi), %xmm0
	pxor %xmm7, %xmm3
	pxor %xmm6, %xmm1
	pxor %xmm3, %xmm12
	pxor %xmm1, %xmm12
	pxor %xmm2, %xmm7
	pxor %xmm0, %xmm6
	movdqa %xmm7, %xmm2
	movdqa %xmm6, %xmm0
	pxor %xmm9, %xmm7
	pxor %xmm9, %xmm6
	aesenc %xmm8, %xmm7
	aesenc %xmm8, %xmm6
	aesenc %xmm9, %xmm7
	aesenc %xmm9, %xmm6
	aesenc %xmm10, %xmm7
	aesenc %xmm10, %xmm6
	aesenc %xmm14, %xmm7
	aesenc %xmm14, %xmm6
	pxor %xmm3, %xmm7
	pxor %xmm1, %xmm6
	movdqa %xmm7, %xmm3
	movdqa %xmm6, %xmm1
	pxor 0(%rcx), %xmm7
	pxor 0(%rcx), %xmm6
	pxor %xmm13, %xmm7
	pxor %xmm13, %xmm6
	pxor 0(%r8), %xmm7
	pxor 16(%r8), %xmm6
	aesenc %xmm8, %xmm7
	aesenc %xmm8, %xmm6
	aesenc %xmm9, %xmm7
	aesenc %xmm9, %xmm6
	aesenc %xmm10, %xmm7
	aesenc %xmm10, %xmm6
	aesenc %xmm14, %xmm7
	aesenc %xmm14, %xmm6
	pxor %xmm7, %xmm2
	pxor %xmm6, %xmm0
	movdqu %xmm2, 0(%rdi)
	movdqu %xmm3, 16(%rdi)
	movdqu %xmm0, 32(%rdi)
	movdqu %xmm1, 48(%rdi)
	addq $64, %rdi
	addq $2, %r10
	subq $64, %rax
1022: # process_32bytes:
	addq $64, %rax
	subq $32, %rax
	jb 1059f # out
	andq $7, %r10
	shlq $4, %r10
	addq %r10, %r9
	movdqa %xmm11, %xmm7
	pxor %xmm13, %xmm7
	pxor 0(%r9), %xmm7
	aesenc %xmm8, %xmm7
	aesenc %xmm9, %xmm7
	aesenc %xmm10, %xmm7
	aesenc %xmm14, %xmm7
	movdqu 0(%rdi), %xmm3
	movdqu 16(%rdi), %xmm2
	pxor %xmm7, %xmm3
	pxor %xmm3, %xmm12
	pxor %xmm2, %xmm7
	movdqa %xmm7, %xmm2
	pxor %xmm9, %xmm7
	aesenc %xmm8, %xmm7
	aesenc %xmm9, %xmm7
	aesenc %xmm10, %xmm7
	aesenc %xmm14, %xmm7
	pxor %xmm3, %xmm7
	movdqa %xmm7, %xmm3
	pxor 0(%rcx), %xmm7
	pxor %xmm13, %xmm7
	pxor 0(%r9), %xmm7
	aesenc %xmm8, %xmm7
	aesenc %xmm9, %xmm7
	aesenc %xmm10, %xmm7
	aesenc %xmm14, %xmm7
	pxor %xmm7, %xmm2
	movdqu %xmm2, 0(%rdi)
	movdqu %xmm3, 16(%rdi)
1059: # out:
	movdqu %xmm12, 0(%rsi)
	movdqa %xmm14, 0(%rsp)
	movdqa %xmm14, 32(%rsp)
	movdqa %xmm14, 64(%rsp)
	movdqa %xmm14, 96(%rsp)
	movq %r11, %rsp
	ret
#ifndef __APPLE__
.size aezCorePass2AMD64AESNI, .-aezCorePass2AMD64AESNI
#endif /* !__APPLE__ */
